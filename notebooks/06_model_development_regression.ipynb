{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d323ff62",
   "metadata": {},
   "source": [
    "# Step 8: Model Development (Regression)\n",
    "\n",
    "Goal: Train baseline + stronger regression models for sales forecasting using a time-aware workflow, compare performance on a held-out test window, and save a deployment-ready pipeline.\n",
    "\n",
    "Rules followed:\n",
    "- Time-based validation (no shuffling)\n",
    "- Deterministic runs (fixed random state)\n",
    "- Outputs and saved reports are ASCII-only\n",
    "- Text files written with UTF-8 encoding\n",
    "\n",
    "Primary holdout metrics:\n",
    "- MAE, RMSE, MAPE (safe), R2\n",
    "\n",
    "Reference formulas:\n",
    "\n",
    "$$\\text{RMSE}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2}$$\n",
    "$$\\text{MAE}=\\frac{1}{n}\\sum_{i=1}^n |y_i-\\hat{y}_i|$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7b1a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook CWD: C:\\Projects\\FUTURE_ML_01\\notebooks\n",
      "Project root: C:\\Projects\\FUTURE_ML_01\\notebooks\n",
      "Data dir: C:\\Projects\\FUTURE_ML_01\\notebooks\\data\n",
      "Models dir: C:\\Projects\\FUTURE_ML_01\\notebooks\\models\n",
      "Reports dir: C:\\Projects\\FUTURE_ML_01\\notebooks\\reports\n",
      "CONFIG: {'use_log_target': True, 'clip_negative_predictions_to_zero': True, 'primary_metric': 'rmse', 'tscv_splits': 3, 'random_state': 42, 'n_jobs': -1}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "import joblib\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 140)\n",
    "\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / 'data').exists() and (p / 'models').exists() and (p / 'reports').exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "\n",
    "NOTEBOOK_CWD = Path.cwd().resolve()\n",
    "PROJECT_ROOT = find_project_root(NOTEBOOK_CWD)\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "REPORTS_DIR = PROJECT_ROOT / 'reports'\n",
    "\n",
    "for d in (DATA_DIR, MODELS_DIR, REPORTS_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    'use_log_target': True,\n",
    "    'clip_negative_predictions_to_zero': True,\n",
    "    'primary_metric': 'rmse',\n",
    "    'tscv_splits': 3,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "print('Notebook CWD:', NOTEBOOK_CWD)\n",
    "print('Project root:', PROJECT_ROOT)\n",
    "print('Data dir:', DATA_DIR)\n",
    "print('Models dir:', MODELS_DIR)\n",
    "print('Reports dir:', REPORTS_DIR)\n",
    "print('CONFIG:', CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c821f936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions ready.\n"
     ]
    }
   ],
   "source": [
    "def safe_mape(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-8) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
    "\n",
    "\n",
    "def evaluate_regression(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    mape = safe_mape(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        'mae': float(mae),\n",
    "        'rmse': float(rmse),\n",
    "        'mape_percent': float(mape),\n",
    "        'r2': float(r2),\n",
    "    }\n",
    "\n",
    "\n",
    "def clip_preds(y_pred: np.ndarray) -> np.ndarray:\n",
    "    if not CONFIG.get('clip_negative_predictions_to_zero', True):\n",
    "        return y_pred\n",
    "    return np.maximum(y_pred, 0.0)\n",
    "\n",
    "\n",
    "def write_text(path: Path, text: str) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(text, encoding='utf-8', newline='\\n')\n",
    "\n",
    "\n",
    "def timer() -> float:\n",
    "    return time.perf_counter()\n",
    "\n",
    "\n",
    "def elapsed_seconds(start: float) -> float:\n",
    "    return float(time.perf_counter() - start)\n",
    "\n",
    "\n",
    "print('Helper functions ready.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17757775",
   "metadata": {},
   "source": [
    "## Load prepared train/test (preferred)\n",
    "\n",
    "This project already persisted the Step 7 artifacts. We will use them for modeling and also load the saved train/test dates to support time-aware plots and checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc4a564",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Missing required Step 7 artifacts: C:\\Projects\\FUTURE_ML_01\\notebooks\\data\\X_train.csv, C:\\Projects\\FUTURE_ML_01\\notebooks\\data\\X_test.csv, C:\\Projects\\FUTURE_ML_01\\notebooks\\data\\y_train.csv, C:\\Projects\\FUTURE_ML_01\\notebooks\\data\\y_test.csv, C:\\Projects\\FUTURE_ML_01\\notebooks\\data\\train_dates.csv, C:\\Projects\\FUTURE_ML_01\\notebooks\\data\\test_dates.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m missing = [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m required_paths \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p.exists()]\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mMissing required Step 7 artifacts: \u001b[39m\u001b[33m'\u001b[39m + \u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m missing))\n\u001b[32m     13\u001b[39m X_train = pd.read_csv(X_TRAIN_PATH)\n\u001b[32m     14\u001b[39m X_test = pd.read_csv(X_TEST_PATH)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Missing required Step 7 artifacts: C:\\Projects\\FUTURE_ML_01\\notebooks\\data\\X_train.csv, C:\\Projects\\FUTURE_ML_01\\notebooks\\data\\X_test.csv, C:\\Projects\\FUTURE_ML_01\\notebooks\\data\\y_train.csv, C:\\Projects\\FUTURE_ML_01\\notebooks\\data\\y_test.csv, C:\\Projects\\FUTURE_ML_01\\notebooks\\data\\train_dates.csv, C:\\Projects\\FUTURE_ML_01\\notebooks\\data\\test_dates.csv"
     ]
    }
   ],
   "source": [
    "X_TRAIN_PATH = DATA_DIR / 'X_train.csv'\n",
    "X_TEST_PATH = DATA_DIR / 'X_test.csv'\n",
    "Y_TRAIN_PATH = DATA_DIR / 'y_train.csv'\n",
    "Y_TEST_PATH = DATA_DIR / 'y_test.csv'\n",
    "TRAIN_DATES_PATH = DATA_DIR / 'train_dates.csv'\n",
    "TEST_DATES_PATH = DATA_DIR / 'test_dates.csv'\n",
    "\n",
    "required_paths = [X_TRAIN_PATH, X_TEST_PATH, Y_TRAIN_PATH, Y_TEST_PATH, TRAIN_DATES_PATH, TEST_DATES_PATH]\n",
    "missing = [p for p in required_paths if not p.exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError('Missing required Step 7 artifacts: ' + ', '.join(str(p) for p in missing))\n",
    "\n",
    "X_train = pd.read_csv(X_TRAIN_PATH)\n",
    "X_test = pd.read_csv(X_TEST_PATH)\n",
    "\n",
    "y_train = pd.read_csv(Y_TRAIN_PATH).squeeze('columns')\n",
    "y_test = pd.read_csv(Y_TEST_PATH).squeeze('columns')\n",
    "\n",
    "train_dates = pd.read_csv(TRAIN_DATES_PATH)\n",
    "test_dates = pd.read_csv(TEST_DATES_PATH)\n",
    "\n",
    "# Normalize date column name (Step 7 saved a single date column)\n",
    "train_date_col = train_dates.columns[0]\n",
    "test_date_col = test_dates.columns[0]\n",
    "train_dates_series = pd.to_datetime(train_dates[train_date_col])\n",
    "test_dates_series = pd.to_datetime(test_dates[test_date_col])\n",
    "\n",
    "print('X_train:', X_train.shape, 'X_test:', X_test.shape)\n",
    "print('y_train:', y_train.shape, 'y_test:', y_test.shape)\n",
    "print('Train dates range:', train_dates_series.min(), 'to', train_dates_series.max())\n",
    "print('Test dates range:', test_dates_series.min(), 'to', test_dates_series.max())\n",
    "\n",
    "# Leakage sanity check (strictly earlier train than test)\n",
    "assert train_dates_series.max() < test_dates_series.min(), 'Time split violation: train overlaps test'\n",
    "assert len(X_train) == len(y_train) == len(train_dates_series)\n",
    "assert len(X_test) == len(y_test) == len(test_dates_series)\n",
    "\n",
    "print('Time split checks passed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c67baa",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline (ColumnTransformer)\n",
    "\n",
    "We build a deployment-ready preprocessing step that can handle numeric and categorical columns safely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169fd1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_feature_types(df_features: pd.DataFrame) -> tuple[list[str], list[str]]:\n",
    "    numeric_cols = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = [c for c in df_features.columns if c not in numeric_cols]\n",
    "    return numeric_cols, categorical_cols\n",
    "\n",
    "\n",
    "def make_preprocess(scale_numeric: bool) -> ColumnTransformer:\n",
    "    numeric_cols, categorical_cols = infer_feature_types(X_train)\n",
    "\n",
    "    numeric_steps = [\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "    ]\n",
    "    if scale_numeric:\n",
    "        numeric_steps.append(('scaler', StandardScaler()))\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=numeric_steps)\n",
    "\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "        ],\n",
    "        remainder='drop',\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "\n",
    "    return preprocess\n",
    "\n",
    "\n",
    "def assert_schema_compatible(train_df: pd.DataFrame, test_df: pd.DataFrame) -> None:\n",
    "    missing_in_test = [c for c in train_df.columns if c not in test_df.columns]\n",
    "    extra_in_test = [c for c in test_df.columns if c not in train_df.columns]\n",
    "    if missing_in_test or extra_in_test:\n",
    "        raise ValueError(\n",
    "            'Schema mismatch between train and test. '\n",
    "            f'missing_in_test={missing_in_test}, extra_in_test={extra_in_test}'\n",
    "        )\n",
    "\n",
    "\n",
    "assert_schema_compatible(X_train, X_test)\n",
    "print('Schema check passed. Columns:', len(X_train.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e8448",
   "metadata": {},
   "source": [
    "## Target transform (optional)\n",
    "\n",
    "Sales can be heavy-tailed. We optionally use a log1p transform during training and inverse it during prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b3587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_wrap_target_transform(regressor: Pipeline | object) -> object:\n",
    "    if not CONFIG.get('use_log_target', True):\n",
    "        return regressor\n",
    "\n",
    "    # log1p is valid for non-negative targets; sales should be >= 0\n",
    "    return TransformedTargetRegressor(\n",
    "        regressor=regressor,\n",
    "        func=np.log1p,\n",
    "        inverse_func=np.expm1,\n",
    "        check_inverse=False,\n",
    "    )\n",
    "\n",
    "\n",
    "print('Target transform enabled:', CONFIG['use_log_target'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e287c9c",
   "metadata": {},
   "source": [
    "## Baseline + model runner\n",
    "\n",
    "We use a single runner that:\n",
    "- fits on the training window only\n",
    "- evaluates on the untouched test window\n",
    "- records metrics and training time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fb20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelResult:\n",
    "    name: str\n",
    "    metrics: dict\n",
    "    train_seconds: float\n",
    "    model: object\n",
    "    notes: str = ''\n",
    "\n",
    "\n",
    "def fit_evaluate_holdout(name: str, estimator: object, Xtr: pd.DataFrame, ytr: pd.Series, Xte: pd.DataFrame, yte: pd.Series, notes: str = '') -> ModelResult:\n",
    "    start = timer()\n",
    "    estimator.fit(Xtr, ytr)\n",
    "    train_s = elapsed_seconds(start)\n",
    "\n",
    "    y_pred = estimator.predict(Xte)\n",
    "    y_pred = clip_preds(np.asarray(y_pred, dtype=float))\n",
    "\n",
    "    metrics = evaluate_regression(yte, y_pred)\n",
    "    return ModelResult(name=name, metrics=metrics, train_seconds=train_s, model=estimator, notes=notes)\n",
    "\n",
    "\n",
    "def make_pipeline(model: object, scale_numeric: bool) -> object:\n",
    "    preprocess = make_preprocess(scale_numeric=scale_numeric)\n",
    "    pipe = Pipeline(steps=[('preprocess', preprocess), ('model', model)])\n",
    "    return maybe_wrap_target_transform(pipe)\n",
    "\n",
    "\n",
    "results: list[ModelResult] = []\n",
    "\n",
    "baseline = make_pipeline(LinearRegression(), scale_numeric=True)\n",
    "results.append(fit_evaluate_holdout('LinearRegression', baseline, X_train, y_train, X_test, y_test, notes='baseline'))\n",
    "\n",
    "print('Baseline metrics:', results[-1].metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
